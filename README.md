# GEM Polaris ECE 484 Team Kachow

⚠️ Disclaimer: You cannot actually run this locally unless you have access to a UIUC ubuntu machine in the ECE 484 classroom. This README file rather shows the results of our project. 

## Abstract

This is a project in which we test out a pipeline which we run using ROS on a GEM Polaris Vehicle. This pipeline includes step-by-step processing of ROS subscriber images generated by ZED2 Camera output to publishing steering angle. This also includes algorithms on how we publish acceleration values and how we maneuver through complex environments such as rainy and snowy conditions.

## Steering and Pedestrian Detection Pipeline
Below shows an image of the pipeline we use to processing our ZED2 Camera output. Once processed, we calculate an approximate steering angle for which the car should turn. 

<p align="center">
  <img src="https://github.com/user-attachments/assets/1fdfc66e-d266-4b7c-8fc2-adc27915d1ef" alt="drawing" width="600"/>
</p>

### ① Bilateral Filter
The Bilateral Filter acts just like a gaussian filter which is neat for filtering in various environmental conditions. Each pixel in the image recieved from the ZED2 camera from the rgb subscriber will utilize neighboring pixels to estimate what its new pixel value should be, which in turn blurs the image. Any snow or rain droplets that faces the camera will average itself out and blend with the image.

### ② YoloPV2

⚠️ Disclaimer: We outsourced the YoloPV2 model from [here](https://github.com/CAIC-AD/YOLOPv2)

The blurred output of the Bilateral Filter is then sent into a YoloPV2 model which performs pedestrian and lane detection. Below shows an example output image of the YoloPV2 model which we captured. Note that the image is blurred which was the results of the previous bilateral filter.

<p align="center">
  <img src="https://github.com/user-attachments/assets/5047e8aa-7db9-4849-8b52-eca8ccda0209" alt="drawing" width="45%"/>
</p>

Note that with the YoloPV2 model, we can also perform pedestrian detection. We take our lidar measurments from our lidar subscriber topic, and perform a depth mapping on the objects we retrieved. If there is a pedestrian that is detected in front, we will publish values to the acceleration topic to stop the vehicle. 

⭐ Click [here](https://drive.google.com/file/d/1pjq8oPXD1aFPnZKNYCzj8fU4euQA-8Hh/view?usp=sharing) to see a demonstration of the pedestrian detection with lidar in action

### ③ Color Gradient Thresholding

With the output of the YoloPV2 model, we needed to extract the lanes itself which are marked in red. Therefore, we can filter for red colors using HLS Color Thresholding. Its not enough to filter for Red since the output is transparent and we can see the lanes itself below the red markings as depicted in the YoloPV2 output. I also want to mention that we turned off gradient filtering. Gradient filtering compares 2 pixels side by side to measure a change in the pixel value. If there is high pixel difference, the pixel will be marked. However, we found out that the gradient thresholding did very little to improve the output, so to increase performance, we did not run the gradient thresholding function. The implementation, however, is still within the repository. After performing the thresholding, we produced output image such as below.  

<p align="center">
  <img src="https://github.com/user-attachments/assets/3043d006-36c7-4ac2-8220-9149ca77c0ad" alt="drawing" width="45%"/>
</p>

### ④ Perspective Transform

With the output of the color thresholding, we can transform our view to help determine our steering angle. In order to do so, we have to undertand how perspectives work. If we look at a straight road, the road itself will look like a trapezoid where the top right corner of the road will be closer to the top left corner of the road compared to the bottom right and bottom left. Therefore, we designed to create our own trapezoid and place it on a the outer lane as shown on the left image below. We then took all of the contents within the trapezoid, and created a 2d top-down view as shown in the image to the right. With this new image, we can determine whether the road is turning left or right.

<p align="center">
  <img src="https://github.com/user-attachments/assets/2695730b-7837-4f91-b42a-477ecd6ae1a1" width="45%" />
  <img src="https://github.com/user-attachments/assets/0fd3ce67-bdd3-445f-add5-7a558ebdc0eb" width="46%" />
</p>

### ⑤ Steering Angle

The last step simply finds the angle between the top right most corner and the bottom left most corner of the lane. Depending on the angle, we will be publishing an angle to a ros topic dedicated for changing the wheel angle. Note that the implementation of determining the angle is done within the "front2steer" function. Below shows an example image depicting the angle formed by the corners. 

<p align="center">
  <img src="https://github.com/user-attachments/assets/4a8ed2a7-d170-43e9-8d80-8c757dab680a" alt="drawing" width="45%"/>
</p>

⭐ Click [here](https://drive.google.com/file/d/1-gOBF6FZbwREzLmjqInui9nwMs97lKg-/view?usp=sharing) to see a steering in action

### ⑥ Decision Tree

Below shows the decision tree for how exactly our Lane + Pedestrian Detection Algorithm works

<p align="center">
  <img src="https://github.com/user-attachments/assets/442f4d5b-7122-4d80-a11b-5c99bf9501f2" alt="drawing" width="45%"/>
</p>


